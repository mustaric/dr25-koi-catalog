\label{s:robovetter}
The dispositioning of the TCEs into the categories of PC and FP is entirely automated and is performed by a piece of code called the Robovetter\footnote{GITHUB link}. This code takes as input a variety of metrics that evaluate the TCEs in different ways and then imposes a set of logic to disposition the TCEs.  In this section we describe the different metrics as well as the logic the Robovetter uses to make its decisions. 

Because the TCE population changed significantly between DR24 and DR25, the Robovetter had to be improved in order to obtain an acceptable performance.  Also, because we now have a population of known false alarms (\invtce\ and \scrtce), the Robovetter could be tuned by balancing its completeness against its effectiveness at removing known false alarms. This is change from previous KOI catalogs that prioritized completeness above all else.  In order to sufficiently remove the long period excess of false alarms, this Robovetter introduces new metrics that evaluate individual transits. This expands the work that the code Marshall \citep{Mullally2015c} performed for the DR24 KOI catalog. 

While many metrics did not change between the DR24 Robovetter and the DR25 Robovetter, we discuss all the metrics used by the DR25 Robovetter below in order to be complete and prevent unnecessary confusion. 



%Move to the introduction
%In the first five \Kepler{} planet candidate catalogs \citep{Borucki2011a,Borucki2011b,Batalha2013,Burke2014,Rowe2015a}, various plots and diagnostics for each TCE were visually examined by members of the Threshold Crossing Event Review Team (TCERT), which consists of professional scientists who have a thorough understanding of \kepler{} data systematics and the various types of false positive scenarios. In the sixth catalog, \citet{Mullally2015cat} employed partial automation through the use of three simple parameter cuts, principally to cull out a large number of long-period false positives, as well as a robotic procedure to identify a particular subset of centroid offsets \citep[see \S5.2 of][]{Mullally2015cat}. In the seventh catalog, \citet{Coughlin2016} fully automated the dispositioning of TCEs in order to enable the accurate computation of planet occurrence rates.


% TALK ABOUT THE vDR24 ROBOVETTER RESULTS??
% \subsubsection{Using the DR24 Robovetter}
%  We applied the DR24 robovetter to the DR25 TCEs. 
%  We found that we needed to do better in the following areas.
%  Don't forget to cite \citet{Tange2011a}.
%  \input{applydr24rv.tex}


% OLD TEXT JUST SAVING FOR NOW
% which requires that every TCE be dispositioned in a uniform manner so that it can be subjected to quantitative evaluation. 
%
% As manual inspection by TCERT members is very time-consuming, it is often not feasible to examine each of the $\sim$20,000 TCEs produced by the \kepler{} pipeline. While TCERT members are well-trained, as humans they do not always agree with each other, and individuals may disposition a given TCE differently depending on external factors such as the time of day, their mood, other TCEs examined recently, etc. However, humans are naturally adept at pattern recognition and categorization, and TCERT has developed an efficient and comprehensible workflow procedure, based on understood physical processes, while working on the previous six planet candidate catalogs. 
%
% Thus, for automating the TCE dispositioning process, we have specifically chosen a robotic vetting procedure that operates via a series of simple decision trees. Hereafter referred to as the ``robovetter'', it attempts to mimic the well-known human vetting process, providing a specific reason for dispositioning any TCE as a false positive. The robovetter was initially developed based on the results of the Q1--Q16 catalog \citep{Mullally2015cat} and then further refined based on the results of manual checks on the the Q1--Q17~DR24 dataset by TCERT members.

\subsection{Major False Positive Flags}
\label{s:majorflags}
Similar to \citet{Rowe2015a}, \citep{Mullally2015cat}, and \citet{Coughlin2016}, we assign FP TCEs to one or more of the following false positive categories:


\begin{itemize}
  \item ``Not Transit-Like'': a TCE whose light curve is not consistent with that of a transiting planet or eclipsing binary, such as instrumental artifacts and non-eclipsing variable stars.
  \item ``Stellar Eclipse'': a TCE that is observed to have a significant secondary event, transit shape, or out-of-eclipse variability that indicates the transit-like event is most likely caused by an eclipsing binary. (Self-luminous, hot Jupiters with a visible secondary eclipse are also in this category, but are still given a disposition of PC.)
  \item ``Centroid Offset'': a TCE whose signal is observed to originate on a nearby star, rather than the target star, based on examination of the pixel-level data.
  \item ``Ephemeris Match Indicates Contamination'': a TCE that has the same period and epoch as another object, and is not the true source of the signal given the relative magnitudes, locations, and signal amplitudes of the two objects.
\end{itemize}

\noindent In Figure~\ref{robovetter-overview-fig} we present a flowchart that outlines our robotic vetting procedure. As can be seen, each TCE is subjected to a series of ``yes'' or ``no'' questions (represented by diamonds) that either disposition it into one or more of the four FP categories, or else disposition it as a PC. Behind each question is a series of more specific questions, each answered by quantitative tests. 


%TALK ABOUT USE OF OPTIMIZATION HERE AND COMPLENESS/RELIABILITY TRADEOFF IN ORDER TO ESTABLISH THRESHOLDS??  This has been placed after the robovetter section.


% These tests are designed with the same ``innocent until proven guilty'' approach that was used by TCERT members in previous catalogs, such that no TCE is dispositioned as a FP without substantial evidence. Quantitatively we are aiming to preserve at least $\sim$95\% of injected transits while rejecting as many false positives as possible.


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{RoboVetter-Diagram-V4-Overview.pdf}
\caption{Overview flowchart of the robovetter. Diamonds represent ``yes'' or ``no'' decisions that are made with quantitative metrics. A TCE is dispositioned as a FP if it fails any test (a ``yes'' decision) and is placed in one or more of the FP categories. If a TCE passes all tests (a ``no'' decision for all tests) it is dispositioned as a PC. The section numbers on each component correspond to the sections in this paper where these tests are discussed. ***UPDATE SECTION HEADERS WHEN PAPER IS NEAR FINAL***. More in-depth flowcharts are provided for the not transit-like and significant secondary modules in Figures~\ref{robovetter-transitlike-fig} and \ref{robovetter-sigsec-fig}.}
\label{robovetter-overview-fig}
\end{figure*}


The Robovetter first checks if the TCE corresponds to a secondary eclipse associated with an already examined system. If not, the Robovetter then checks if the TCE is transit-like or not. If it is transit-like, the Robovetter then looks for the presence of a secondary eclipse. In parallel, the Robovetter also looks for evidence of a centroid offset and an ephemeris match to other TCEs and variable stars in the \kepler{} field. In the following subsections we describe in detail each of these tests in the order in which they are performed by the robovetter. Finally, we explain the creation of a ``disposition score'' that conveys the confidence in the Robovetter's disposition.


\subsection{Two Robovetter Detrendings}
We note that for all of the Robovetter tests that require a phased light curve and model fit, we utilize two different detrendings and model fits. In the \kepler{} pipeline, the Data Validation (DV) module produces a harmonic-removed, median-detrended, phased flux light curve, along with a transit model fit \citep{Wu2010}. However, the harmonic remover is known to suppress short-period ($\lesssim$ 3 days) signals such that short-period eclipsing binaries with visible secondaries can appear as transiting planets with no visible secondary \citep{Christiansen2013b}. It can also make variable stars with semi-coherent variability, such as starspots or pulsations, appear as transit-like signals. Thus, we create phased flux light curves via an alternate detrending method that utilizes the pre-search data conditioned (PDC) time-series light curves and the non-parametric penalized least squares detrending method of \citet{Garcia2010}, which includes only the out-of-transit points when computing the filter. These alternately (ALT) detrended light curves are then phased and fit with a simple trapezoidal transit model. This alternate detrending technique is effective at accurately detrending short-period eclipsing binaries and variable stars, i.e., preserving their astrophysical signal. Every test that is applied to the DV phased light curves is also applied to the alternate detrending --- failing a test using either detrending results in the TCE being classified as a FP.

\subsection{The TCE is the Secondary of an Eclipsing Binary}

If a TCE under examination is not the first one in a system, the Robovetter checks if there exists a previous TCE with a similar period that was designated as a FP due to a significant secondary (see~\S\ref{sigsecsec}). To compute whether two TCEs have the same period within a given statistical threshold, we employ the period matching criteria of \citet[][see equations 1-3]{Coughlin2014a}, $\sigma_{P}$, where higher values of $\sigma_{P}$ indicate more significant period matches. We re-state the equations here as:

\begin{equation}
\label{peq1}
\Delta P = \frac{P_{A}-P_{B}}{P_{A}}\\
\end{equation}

\begin{equation}
\label{peq2}
\Delta P^{\prime} = \textrm{abs}(\Delta P - \textrm{rint}(\Delta P))\\
\end{equation}

\begin{equation}
\label{peq3}
\sigma_{P} = \sqrt{2}\cdot\textrm{erfcinv}(\Delta P^{\prime})\\
\end{equation}

\noindent where $P_{A}$ is the period of the shorter-period TCE, $P_{B}$ is the period of the longer-period TCE, $rint()$ rounds a number to the nearest integer, $abs()$ yields the absolute value, and erfcinv() is the inverse complementary error function. We consider any value of $\sigma_{P}$ $>$ 3.5 to indicate significantly similar periods.

If the current TCE is (1) in a system that has a previous TCE dispositioned as a FP due to a significant secondary, (2) matches the previous TCE's period with $\sigma_{P}$ $>$ 3.5, and (3) is separated in phase from the previous TCE by at least 2.5 times the transit duration, then the current TCE is considered to be a secondary eclipse. In this case, it is designated as a FP and is classified into both the not transit-like and significant secondary FP categories --- a unique combination that can be used to identify secondary eclipses while still ensuring they are not assigned \kepler{} Object of Interest numbers (see \S\ref{koisec}). Note that since the \kepler{} pipeline identifies TCEs in order of their SNR, from high to low, sometimes a TCE identified as a secondary can have a deeper depth than the primary, depending on their relative durations and shapes.

There are two cases where we modify the three criteria above. First, it is possible that the periods of two TCEs will meet the period matching criteria, but be different enough to have their relative phases shift significantly over the $\sim$4 year mission duration. Thus, the potential secondary TCE is actually required to be separated in phase by at least 2.5 times the previous TCE's transit duration over the entire mission time frame in order to be labeled as a secondary. Second, the \kepler{} pipeline will occasionally detect the secondary eclipse of an EB at a half, third, or some smaller integer fraction of the orbital period of the system, such that the epoch of the detected secondary coincides with that of the primary. Thus, for the non-1:1 period ratio cases, we do not impose the phase separation requirement. (Note that equations~\ref{peq1}-\ref{peq3} allow for integer period ratios.)



\subsection{Not Transit-Like}
\label{nottransitlikesec}

A very large fraction of false positive TCEs have light curves that do not resemble a detached transiting or eclipsing object. These include quasi-sinusoidal light curves from pulsating stars, starspots, and contact binaries, as well as more sporadic light curves due to instrumental artifacts. The first step in the catalog process is to determine whether each TCE is not transit-like, or if it should be given \kepler{} Object of Interest (KOI) numbers, which are used to keep track of transit-like systems over multiple \kepler{} pipeline runs. We thus employ a series of algorithmic tests to reliably identify these not transit-like FP TCEs, as shown by the flowchart in Figure~\ref{robovetter-transitlike-fig}.


\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{RoboVetter-Diagram-V4-TransitLike.pdf}
\caption{Not transit-like flowchart of the robovetter. Diamonds represent ``yes'' or ``no'' decisions that are made with quantitative metrics. If a TCE fails any test (via a ``yes'' response to any decision) then it is dispositioned as a not transit-like FP. If a TCE passes all tests (via a ``no'' response to all decisions), then it is given a KOI number and passed to the significant secondary module (see \S\ref{sigsecsec} and Figure~\ref{robovetter-sigsec-fig}). The section numbers on each decision diamond correspond to the sections in this paper where these tests are discussed.}
\label{robovetter-transitlike-fig}
\end{figure*}


\subsubsection{Not Transit-Shaped}

One of the most common false positives in the data set are quasi-sinusoidal shaped light curves, compared to those that are more detached due to a transit or eclipse. Also, individual events can often be due a systematic feature, such as a sudden discontinuity in the light curve. As such, the Robovetter employs tests that detect quasi-sinusoidal light curves and systematics.


\subsubsection{The LPP Metric}
\label{s:lpp}

Many short-period false positives are due to variable stars that exhibit a quasi-sinusoidal phased light curve. We implemented the LPP transit-like metric described by \citet{Thompson2015b} to separate those TCEs that show a transit shape from those that do not. This technique bins the TCE's folded light curve and then applies a dimensionality reduction algorithm called Locality Preserving Projections (LPP) \citep{He2004}.  It then measures the average Euclidean distance to the nearest known transit-like TCEs to yield a single number that represents the similarity of a TCE's shape to that of known transits. 

For the DR25 KOI catalog, we deviated slightly from the method described by \citet{Thompson2015b}.  This was done because the original metric was highly dependent on both period and MES. The empirical LPP metric threshold increases at periods less than a few days. This is caused by the fact these TCEs transit for a large fraction of the orbital period and thus have a different transit shape. Because transit injection does not include a large number of this type of event, the LPP values for good transits can be larger. The trend with MES is routed in the fact that when the binned light curve has a lower signal to noise, it is less likely for two folded light curves to be similar to each other, creating more scatter in the final metric. 

To calculate the LPP metric we start with detrended \kepler{} light curves. Note, we calculated the metric for both the DV and ALT detrendings. When training the LPP algorithm, we used the set of recovered transit injection TCEs and those planet candidates from the DR24 KOI catalog \citep{Coughlin2016} that were found as TCEs in DR25.  Because we injected very few short period transits, including the short period DR24 candidates helped reduce the dependence of the LPP metric on period.  Also, we changed how the folded light curve was binned. To remove some of the MES dependence, TCEs with lower MES are given wider bins for those near the transit center; note, this does not change the number of bins. Because it yielded better performance, we used 99 bins in total to represent the shape of the folded light curve (41 bins emphasizing the in-transit portion), used the LPP algorithm to reduce the dimensions down to 12 and compared to the 5 nearest neighbors in the training set.  Finally, we divided these raw LPP values by the 75$^{th}$ percentile of the 100 TCEs that are closest in period. 
%(Did we also do this in MES? -- No. just Period) 
In this way we further remove the period dependence on the LPP metric.  The resulting LPP metric values lie near to a value of one where values greater than $\approx$ 2 do not appear to be transit shaped.  To create the DR25 catalog the Robovetter used a threshold of 2.2 for the DV detrending and 3.2 for the ALT detrending.

%Begin Jeff's words.
%Many short-period false positives are due to variable stars that exhibit a quasi-sinusoidal phased light curve. \citet{Matijevic2012} used a technique known as Local Linear Embedding (LLE), a dimensionality reduction algorithm, to classify the ``detachedness'' of \kepler{} eclipsing binary light curves on a scale of 0 to 1, where 0 represented fully detached systems with well-separated, narrow eclipses and 1 represented contact binaries with completely sinusoidal light curves. We use a similar technique, known as Local Preserving Projections \citep[][LPP]{He2004}, to distinguish transit-like signals from not transit-like signals \citep{Thompson2015b}. LPP returns a single number that represents the similarity of a TCE's shape to that of known transits. Unlike LLE, LPP can be applied to any TCE, not just those that lie within the parameter space of the training set. Thus, LPP is more suitable for separating transit-like TCEs from all other not transit-like TCEs, and can be run on artificially injected transits.


%We then fold and bin each light curve into 141 points, ensuring adequate coverage of both the in- and out-of-transit portions of the light curve. We exclude points near a phase of 0.5, as the presence of a secondary eclipse in a short-period binary may unduly influence the LPP value, and we seek to classify detached eclipsing binaries as transit-like. These 141 points act as the initial number of dimensions that describe each TCE. Using a subset of known transit-like TCEs, we create a map from the initial 141 dimensions down to 20 dimensions. We apply this map to all TCEs and measure the average Euclidean distance of each to the 15 nearest known transit-like TCEs. This average distance is the value of the LPP metric. When small, it means other transit-like TCEs are nearby in the 20 dimensional space and thus is likely to be shaped like a transit. We calculate this LPP transit metric for all TCEs using both the DV and the alternate detrending, as described in \S\ref{nottransitlikesec}.

%SUSAN - DESCRIBE HERE HOW YOU NORMALIZED THE LPP VALUES W.R.T. PERIOD, etc. 

% OLD TEXT DESCRIBING THE CHOICE OF THRESHOLD. Now we just adjust.
%
% In order to quantitatively determine a threshold between transit-like and not transit-like, we run the LPP classifier on both detrendings of the injected transits (see \S\ref{injectsec}), which we know a priori are all transit-shaped, barring any light curve distortion due to detrending. We then fit a Gaussian to the resulting distribution, computing its median and standard deviation. We then select a maximum LPP cutoff such that we expect less than one false negative in \ntces{} TCEs, via 

% \begin{equation}
% \sigma_{\rm LPP} = \sqrt{2}\cdot \textrm{erfcinv}(1/N_{\rm TCEs})
% \end{equation}

% \noindent where $N_{TCEs}$ = \ntces{}, yielding $\sigma_{\rm LPP}$ = 4.06. Any TCE with a LPP value greater than the median plus 4.06 times the standard deviation, using either detrending, is considered not transit-like.



\subsubsection{Sine Wave Event Evaluation Test}
\label{s:sweetntl}

On occasion, a quasi-sinusoidal variable star will appear as transit-like in both the DV and ALT detrendings. In these cases, the only way to detect that the TCE is a FP is by examining the PDC data and looking for a strong quasi-sinusoidal signal at the TCE's period. We thus invented the Sine Wave Event Evaluation Test (SWEET) to identify these types of FPs.

SWEET takes the PDC data and normalizes each quarter by dividing by the median flux value of that quarter, then subtract 1.0. Outliers are then robustly removed by utilizing the median absolute deviation. Three different sine curves are then fit to the resulting data, with their periods fixed to half, exactly, and twice the TCE period, with their phase, amplitude, and offset allowed to vary. Of the three fits, the one with the highest SNR, defined as the amplitude divided by its error, is chosen as the strongest fit. If a TCE has a SWEET SNR greater than 50, an amplitude greater than the TCE transit depth in both the DV and ALT detrendings, and has a period less than 5.0 days, it fails a not transit-like.

% Can provide exact outlier rejection formula if desired, but seemed unnecessary



\subsubsection{Per-TCE CHASES}
\label{s:tcechases}
NEED TEXT FROM CHRIS




\subsubsection{The Model-Shift Uniqueness Test}
%\label{notuniquetcesec}
\label{s:ms}

If a TCE under investigation is truly a PC, there should not be any other transit-like events in the light curve with a depth, duration, and period similar to the primary signal, in either the positive or negative flux directions, i.e., the transit event should be unique in the phased light curve. Many false positives are due to quasi-sinusoidal signals (see \S\ref{tcesec}) and thus are not unique in the phased light curve. In order to identify these cases, TCERT developed a ``model-shift uniqueness test'' and used it extensively for identifying false positives in the Q1--Q12 \citep{Rowe2015a}, Q1--Q16 \citep{Mullally2015cat}, and DR24 \citep{Coughlin2016} planet candidate catalogs.

See \S3.2.2 of \citet{Rowe2015a} and page 20 of \citet{Coughlin2014b} for figures and a detailed explanation of the ``model-shift uniqueness test'', but in brief, after removing outliers, the best-fit model of the primary transit is used as a template to measure the best-fit depth of the transit model at all other phases. The deepest event aside from the primary (pri) transit event is labeled as the secondary (sec) event, the next-deepest event is labeled as the tertiary (ter) event, and the most positive (pos) flux event (i.e., shows a flux brightening) is labeled as the positive event. The significances of these events ($\sigma_{\rm Pri}$, $\sigma_{\rm Sec}$, $\sigma_{\rm Ter}$, and $\sigma_{\rm Pos}$) are computed assuming white noise as determined by the standard deviation of the light curve residuals. Also, the ratio of the red noise (at the timescale of the transit duration) to the white noise ($F_{\rm Red}$) is computed by examining the standard deviation of the best-fit depths at phases outside of the primary and secondary events.  

When examining all events among all TCEs, assuming Gaussian noise, the minimum threshold for an event to be considered statistically significant is given by

\begin{equation}
    FA_{1} = \sqrt{2}\cdot \textrm{erfcinv}\left(\frac{T_{\rm dur}}{P \cdot N_{\rm TCEs}}\right)
\end{equation}

\noindent where $T_{\rm dur}$ is the transit duration, and $P$ is the period. (The quantity $P$/$T_{\rm dur}$ represents the number of independent statistical tests for a single target.) When comparing two events from the same TCE, the minimum difference in their significances in order to be considered distinctly different is given by

\begin{equation}
    FA_{2} = \sqrt{2}\cdot \textrm{erfcinv}\left(\frac{T_{\rm dur}}{P}\right)
\end{equation}

\noindent We thus compute the following quantities to use as decision metrics

\begin{equation}
    MS_{1} = FA_{1} - \sigma_{\rm Pri}/F_{\rm Red}
\end{equation}

\begin{equation}
    MS_{2} = FA_{2} - (\sigma_{\rm Pri} - \sigma_{\rm Ter})
\end{equation}

\begin{equation}
    MS_{3} = FA_{2} - (\sigma_{\rm Pri} - \sigma_{\rm Pos})
\end{equation}

In the robovetter, we disposition a TCE as a not transit-like FP if either $MS_{1}$~$>$~1.0, $MS_{2}$~$>$~2.0, or $MS_{3}$~$>$~4.0 in the DV detrending, or if either $MS_{1}$~$>$~-3.0, $MS_{2}$~$>$~1.0, or $MS_{3}$~$>$~1.0 in the alternate detrending. These criteria ensure that the primary event is statistically significant when compared to the systematic noise level of the light curve, the tertiary event, and the positive event, respectively. We also fail as not transit-like if $\sigma_{\rm Pri}$ exactly equals zero in both the DV and ALT detrendings, i.e., their default values, which indicates a lack of fit in both detrendings, and thus something fundamentally flawed with the TCE.


\subsubsection{Dominated by Single Event}
\label{s:sesmes}

The depths of individual transits of planet candidates should be equal to each other, and thus assuming constant noise levels, the SNR of individual transits should be nearly equivalent as well. In contrast, most of the long-period FPs that result from three or more equidistant systematic events are dominated in SNR by one of those events. The \kepler{} pipeline measures detection significance via the Multiple Event Statistic (MES), which is calculated by combining the Single Event Statistic (SES) of all the individual events that comprise the TCE --- both the MES and SES are measures of SNR. Assuming all individual events have equal SES values,

\begin{equation}
{\rm MES} = \sqrt{N_{\rm Trans}} \cdot {\rm SES}
\end{equation}

\noindent where $N_{\rm Trans}$ is the number of transit events that comprise the TCE. Thus, SES/MES = 0.577 for a TCE with three transits, and less for a greater number of transits. If the largest SES value of a TCE's transit events, ${\rm SES}_{\rm Max}$, divided by the MES is much larger than 0.577, this indicates that one of the individual events dominates when calculating the SNR.

In the robovetter, for TCEs with periods greater than 90 days, if ${{\rm SES}_{\rm Max} / {\rm MES} > 0.8}$ it is dispositioned as a not transit-like false positive. The period cutoff of 90 days is applied because short-period TCEs can have a large number of individual transit events, which dramatically increases the chance of one event coinciding with a large systematic feature, thus producing a large ${{\rm SES}_{\rm Max} / {\rm MES}}$ value despite being a valid planetary signal.

% The value of 0.8 was empirically chosen based on the results of transit injection (\S\ref{injectsec}) to reject a minimal number of valid planetary candidates, accounting for natural deviations of SES values due to light curve systematics and changes in local noise levels. 


\subsubsection{Previous TCE With Same Period}
\label{s:sameperiod}
Most quasi-sinusoidal false positives produce multiple TCEs at the same period, or at integer ratios of each other. If a TCE in a system has been declared as not transit-like due to another test, it is logical that all subsequent TCEs in that system at the same period, or ratios thereof, should also be dispositioned not transit-like. Thus, we match the period of a given TCE to all previous not transit-like FPs via equations~\ref{peq1}-\ref{peq3}. If the current TCE has a period match with $\sigma_{P}$ $>$ 3.25 to a prior not transit-like FP, it is also dispositioned as a not transit-like FP.

Similarly, some TCEs are produced that correspond to the edge of a previously identified transit-like TCE in the system. This often results when the previous TCE corresponding to a transit or eclipse is not completely removed prior to searching the light curve for another TCE. Thus, we match the period of a given TCE to all previous transit-like TCEs via equations~\ref{peq1}-\ref{peq3}.  If the current TCE has a period match with $\sigma_{P}$ $>$ 3.25 to a prior transit-like FP, and the two epochs are separated in phase by less than 2.5 transit durations, the current TCE is dispositioned as a not transit-like FP. For clarity, we note that it is sometimes possible that the periods of two TCEs will meet the period matching criteria, but be different enough to have their epochs shift significantly in phase over the $\sim$4 year mission duration. Thus, if they are separated in phase by less than 2.5 transit durations at any point in the mission time frame, the current TCE is dispositioned as a not transit-like FP.



\subsubsection{Individual Transit Metrics}
\label{s:indivtrans}
A new approach implemented in DR25 is to identify individual transit events for each TCE that are not actually due to a transit. After rejecting these `bad' transit events, we check if either

\begin{itemize}
\item There are less than 3 `good' events left
\item The re-computed MES using only `good' events is $<$ 7.1
\end{itemize}

\noindent If either of these conditions are met, then the TCE is failed as not transit-like This is in line with the \kepler{} mission requirement of at least three valid transit events with a MES~$\ge$~7.1 in order to generate a TCE. In the following subsections we list the various tests we apply to each individual transit event.


\paragraph{Rubble -- Missing Data}

A number of TCEs from the \kepler{} pipeline are based on transit events that are missing a significant amount of data either in-transit or just before and/or after. These tend to be false positives that are triggering on edges of gaps, or cases were a large amount of data has been removed and a TCE is being created from the residuals of previous TCEs in the system. We thus devised the ``Rubble'' metric to clean-up these remains from the TCE list. The Rubble value for each individual transit is computed by dividing the number of \Kepler\ cadences that are actually available by the number of cadences expected given \Kepler's regular 29.42\,min cadence.  We calculated Rubble across two transit durations centered on the epoch for each transit given the TCE's original linear ephemeris. 


\paragraph{Marshall -- Transit Shape}
\label{marshsec}
\citet{Coughlin2016} used the Marshall method \citep{Mullally2016} to identify and reject false alarm TCEs caused by short period transients in the data. Marshall fits the proposed transit with models of a transit and various transients and used a Bayesian Information Criterion to decide which model was the best explanation for the data. Simulations in \citet{Mullally2016} showed that Marshall was 95\% complete for TCEs with periods $>150$\,days and correctly rejected 66\% of simulated artifact events. The limit on Marshall's effectiveness at eliminating false alarms was it used a parabola to describe the out-of-transit flux, which failed to capture much of the real observed stellar variability. To ensure high completeness, Marshall was tuned to prevent a variable continuum causing true transits to be rejected, at the cost of a lower than ideal effectiveness.

For this catalog, we use a Gaussian Process approach \citep[GP][]{Rasmussen10} to provide an improved continuum model to improve our effectiveness while maintaining our high completeness. Briefly, our approach aims to model the covariance in the light curve to better fit the trends in our data.
A similar approach was used by \citet{ForemanMackey16} to model single transits due to very long period planets ($P > 1000$\,days).

Our procedure is as follows. For each individual proposed transit event, we select a snippet of PDC data 30 times the reported transit duration centered on the event. Where the event happens near the start (or end) of a quarter, we take a snippet of similar length anchored at the start (or end) of the quarter. We use the George package \citep{Ambikasaran14} to fit the covariance of the out-of-transit flux with an exponential squared function, $ {\mathrm{Cov(\delta t})} = A \exp{ (\delta t/\ell)^2}$, where $A$ and $\ell$ are tunable parameters. 

We next fit four models to the entire snippet.

\begin{equation}
\left.\begin{aligned}
G(t | A, \ell) + y_0 \\
G(t | A, \ell) + y_0 + S(t)\\
G(t | A, \ell) + y_0 + S(t)(1 - \exp{\beta t})\\
G(t | A, \ell) + y_0 + S(t - \tau/2) - S(t + \tau/2) 
\end{aligned}\right.
\end{equation}

\noindent
where $G$ is the Gaussian Process model with the tunable parameters held fixed to those found earlier, and $y_0$ is a constant offset. $S(t)$ is given by

\begin{equation}
S(t) = \frac{d}{1 - e^{-\gamma (t-t_0)} }
\end{equation}

\noindent
where $d$ and $t_0$ are tunable parameters, while $\gamma$ is held constant. This function, known as a sigmoid (or logistic) function, has a value of 1 for $t<t_0$, 0 for $t>t_0$, and transitions quickly, but smoothly, between the two states. By using a sigmoid and avoiding the discontinuities in present in the models used by the original Marshall algorithm \citet{Mullally2016} we can use the L-BFGS-B algorithm \citep{Byrd95} available in the Scipy package \footnote{\url{www.scipy.org}} instead of the less robust Neldar-Mead.

The second function models a discrete jump in the data. We fit this model seeded with a negative-going dip at the predicted time of ingress, and also with a positive-going spike at the predicted egress, as we see both features in \Kepler\ data. The third model fits a Sudden Pixel Sensitivity Drop (SPSD) event, probably caused by a cosmic ray hit on the detection. The last model approximates a box transit. By varying the parameter $\gamma$ we could in principle model transit ingress and egress, but find that extra degree of freedom is not necessary to explain the low signal-to-noise events of most concern.

For each transit the Marshall method returns the BIC (Bayesian Information Criterion) score, the preferred model and the difference between the preferred model and the sigmoid box fit.  A transit is considered sufficiently bad when the Marshall score exceeds a particular threshold, as with the original Marshall algorithm.  However, in a few cases the Gaussian processes fails, yield extremely large, unbelievable BIC values. In these cases the transit is set to always pass.  Also, for low MES transits, the expected SES of a transit is sufficiently low that Marshall will be unable to distinguish between the ``no transit" model and a low signal-to-noise transit.  Because of this the Robovetter uses the following logic when deciding whether a specific transit is not valid:



%Jeff's words.
%A number of long-period false positives are a result of three or more systematic events that happen to be equidistant in time and produce a TCE. There are two prominent types of systematic events in \kepler{} data: sudden pixel sensitivity dropouots (SPSDs) and step-wise discontinuities. SPSDs are due to cosmic ray impacts that temporarily reduce the detection sensitivity of the impacted pixels, resulting in a sudden drop in flux followed by an asymptotic rise back to the baseline flux level over a timescale of a few hours \citep{VanCleve2009}. Step-wise discontinuities are sudden jumps in the baseline flux level, in either the positive or negative flux direction, and are typically due to imperfect detrending, but may have other causes. If a TCE is due to several of these events that are of similar SNR, they will not be flagged as false positives without examining the shape of their individual events.

%ARE THERE MORE MODELS THAN JUST STEPWISE AND SPSD? WERE ANY ADDED IN DR25??  WHATS OFFSET ONLY - WE NEED TO DESCRIBE

%In order to detect TCEs due to SPSDs and step-wise discontinuities, we developed the ``Marshall'' metric \citep{Mullally2015b}. Marshall fits a transit, SPSD, and step-wise discontinuity model to each individual event of a long-period TCE. The Bayesian Information Criterion \citep[BIC;][]{Schwarz1978} is then used to select which model best fits each individual transit event given each model's number of degrees of freedom. 

If all of the following criteria are met:

\begin{itemize}
\item The BIC score of the best-fitting non-transit model is at least 10 lower than the BIC of the transit-model
\item The BIC of the best-fitting non-transit model is less than 1.0E6
\item Either MES/NREALTRANS $>$ 4.0 or the lowest BIC model is for the constant offset model, 
\end{itemize}

\noindent then that event is determined to be due to a systematic rather than a transit, and labeled as a ``bad'' event. (NREALTRANS is the number of transit events for the TCE.)

%{\bf NEED TO MAKE SURE "NREALTRANS" IS DEFINED SOMEWHERE, AND OFFSET ONLY}

%{\bf Add some kind of discussion of performance}
{\bf Add link to Source Forge code}


\paragraph{Chases -- SES artifacts}

NEED TEXT FROM CHRIS

Chris on KSO-447: I have only included events where an absolute value of the SES peak within 0.8 of the transit event SES is within 30\% of the analysis window. The analysis window is set to Porb/10.0.

``Chases'' looks at the TPS convolved light curve and measures the amount of ayssmetry in the SES time serires...



\paragraph{Skye -- Image Artifacts Clustered by Skygroup}

% CHECK IF THE TERM 'SKYGROUP' HAS BEEN DEFINED PREVIOUSLY. IF NOT, DEFINE IT HERE.

As discussed in \ref{marshsec}, there are a number of TCEs caused by rolling-band image artifacts. These artifacts appears as reduction in flux that moves across the CCD and therefore affects different targets on the same CCD at about the same time. Thus, if a number of individual transit events from TCEs on different targets, but the same skygroup (region of the sky that falls on the same CCD each quarter), occur at the same time, they are very likely systematic in origin. The metric called ``Skye'' looks for an excess in the number of individual events occurring at the same time in the same skygroup. If an excess is identified we consider these events to be caused by artifacts. 

More specifically, for each skygroup we bin the individual events into 1.0\,d bins. We only use those \opstce\ with periods greater than 45 days ($\sim$half a \Kepler\ quarter) for each skygroup. The reason for the period cut is that the long-period \opstce s are likely to be affected by rolling-band systematics, but the short-period ones are not.  Including shorter period TCEs would dramatically increase the number of individual transits and would reduce the significance of the anomalous peaks.  See Figure~\ref{skyefig} for an example of the anomalous peaks seen when the data is binned in this way.

To determine which events are anomalous, for each skygroup, we compute the average rate ($R$) of transits, by dividing the overall number of transit times in the skygroup by the number of 1.0\,d bins. Assuming the majority of transits are randomly distributed in time, and utilizing Poisson counting statistics, any peaks greater than:

\begin{equation}
threshold = R + N*\sqrt{R}
\end{equation}

\noindent are statistically significant and indicative of temporal clustering, given a chosen value for $N$. We choose a value of $N = 3.0$, and robustly determine the rate for each skygroup by first computing the \emph{threshold} using all the bins, then iteratively rejecting all bins with a height greater than \emph{threshold} and re-computing \emph{threshold} until it converges and does not change with further iterations.

For each skygroup and its threshold, we identify the individual times-of-transit for TCEs belonging to the skygroup that fall in bins that are above the threshold. We assign Skye values of 1 to these individual events to indicate they are 'bad'. The Skye value for all other transit times are set to 0.  



\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{Skye-Paper-Plot.png}
\caption{Skye Example. WE WILL need to update a some point.}
\label{skyefig}
\end{figure}


\paragraph{Zuma -- Negative Significance}

A valid transit-like TCE should be comprised of individual events that correspond to flux decrements. If any event instead shows an increase of flux then that event is suspect. We thus designate any individual event with SES~$<$~0 as `bad'.


\paragraph{Tracker -- Ephemeris Slip}

After the TPS module of the \kepler{} pipline detects a TCE, it is sent to to DV to be fit with a full transit model. DV allows the period and epoch to vary when fitting in order to provide as accurate a fit as possible. Sometimes the TPS ephemeris and DV ephemeris can end up significantly different though. When this occurs it indicates that the underlying data is not transit-like and the TCE is likely due to quasi-sinusoidal systematics, which case the ephemeris to wander when fitting.

``Tracker'' keeps track of the time difference between the TPS and DV linear ephemerides in units of the TCE's duration. When Tracker $>$ 0.5 for any event we designate the event as 'bad'.


% The metric formally known as Rocky
\subsubsection{Fraction of Gapped Events}

Due to the method of data gapping employed in TPS, sometimes the \kepler{} pipeline can create a TCE that has a majority of its individual events occur where there is no actual in-transit data. This tends to happen particularly in multi-TCE systems, because once the \kepler{} Pipeline detects a TCE in a given system, it removes the data corresponding to the in-transit cadences of that TCE, and re-searches the light curve. 

We thus developed a metric that measures the number of individual transit events that actually contain data. Specifically we compute the fraction of individual events with either SES~$\ne$~0 or Rubble~$>$~0.75, which indicate there is sufficient in-cadence data present. If the fraction of transits with data is~$\le$~0.5, then we fail the TCE as not transit-like.



\subsection{Stellar Eclipse}
\label{sigsecsec}

If a TCE is deemed transit-like by passing all of the tests presented in \S\ref{nottransitlikesec} on both detrendings, it is given a KOI number. However, many of these KOIs are FPs due to eclipsing binaries and contamination from nearby variable stars. We employ a series of robotic tests to detect systems that are due to stellar companions, as shown by the flowchart in Figure~\ref{robovetter-sigsec-fig}.

%In order to produce a uniform catalog, we do not designate any TCE a FP on the basis of its transit depth or inferred radius --- see \S7 item 6 of \citet{Mullally2015cat} for more detail. 

% us, being agnostic to stellar parameters, the only way to definitively detect an EB via a \kepler{} light curve is by detecting a significant secondary eclipse. 

MENTION THE WHOLE SECONDARY -> STELLAR CHANGE, OR NOT?




\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{RoboVetter-Diagram-V3-SigSec.pdf}
\caption{Flowchart describing the stellar system tests of the robovetter. Diamonds represent ``yes'' or ``no'' decisions that are made with quantitative metrics. The multiple arrows originating from ``Start'' represent decisions that are made in parallel. NEED TO UPDATE!!}
\label{robovetter-sigsec-fig}
\end{figure*}


\subsubsection{Secondary Eclipse}

One of the most common methods to detect a stellar system is the presence of a significant secondary in the light curve. With the exception of some hot Jupiter type planets, the visibility of a secondary eclipse in \kepler{} data is a telltale sign of a stellar eclipsing binary.


\paragraph{Subsequent TCE With Same Period}

Once the \kepler{} pipeline detects a TCE in a given system, it removes the data corresponding to this event and re-searches the light curve. It is thus able to detect the secondary eclipse of an EB as a subsequent TCE, which will have the same period as, but different epoch than, the primary TCE. Thus, utilizing equations~\ref{peq1}-\ref{peq3}, the robovetter dispositions a TCE as a stellar system FP if its period matches a subsequent TCE within the utilized tolerance ($\sigma_{P}$ $>$ 3.25) and they are separated in phase by at least 2.5 times the transit duration. For clarity, we note again that it is sometimes possible that the periods of two TCEs will meet the period matching criteria, but be different enough to have their epochs shift significantly in phase over the $\sim$4 year mission duration. Thus, this phase separation requirement is required to be upheld over the entire mission duration in order to disposition the TCE as a FP due to a significant secondary.

Occasionally the \kepler{} pipeline will detect the secondary eclipse of an EB at half, third, or some smaller integer fraction of the orbital period of the system. In these cases, the epoch of the TCE corresponding to the secondary will overlap with that of the primary. These cases are accounted for by not requiring a phase separation of at least 2.5 transit durations when a period ratio other than unity is detected. (Note that equations~\ref{peq1}-\ref{peq3} allow for integer period ratios.) While this approach will likely classify any multi-planet system in an exact 2:1 orbital resonance as a FP due to a significant secondary, in practice this is non-existent. Exact 2:1 orbital resonances, where ``exact'' means the period ratio is close enough to 2.0 over the $\sim$4 year mission duration to avoid any drift in relative epoch, appear to be extremely rare \citep{Fabrycky2014}. Also, they would produce strong transit timing variations, which would likely preclude their detection. The \kepler{} pipeline employs a strictly linear ephemeris when searching for TCEs, and thus while planets with mild transit timing variations (TTVs), e.g., deviations from a linear ephemeris less than the transit duration, are often detected, planets with strong TTVs, e.g., deviations from a linear ephemeris greater than the transit duration, are often not detected.



\paragraph{Secondary Detected in Light Curve}
\label{secdetectsec}

There are many cases when a secondary eclipse does not produce its own TCE, most often when its MES is below the \kepler{} pipeline detection threshold of 7.1. The model-shift uniqueness test, discussed in \S\ref{s:ms}, is well-suited to automatically detect secondary eclipses in the phased light curve, as it searches for the next two deepest events aside from the primary event. It is thus able to detect the best-candidate secondary eclipse in the light curve and assess its significance. We thus compute the following quantities to use as secondary deteection metrics

\begin{equation}
    MS_{4} = \sigma_{\rm Sec}/F_{\rm Red} - FA_{1}
\end{equation}

\begin{equation}
    MS_{5} = (\sigma_{\rm Sec} - \sigma_{\rm Ter}) - FA_{2}
\end{equation}

\begin{equation}
    MS_{6} = (\sigma_{\rm Sec} - \sigma_{\rm Pos}) - FA_{2}
\end{equation}

If $MS_{4}$~$>$1, $MS_{5}$~$>$0, and $MS_{6}$~$>$0, in either the DV or alternate detrendings, the robovetter dispositions the TCE as a stellar system FP. These criteria ensure that the secondary event is statistically significant when compared to the systematic noise level of the light curve, the tertiary event, and the positive event, respectively.

There are two exceptions when the above-mentioned conditions are met, but the robovetter does not designate the TCE a false positive. First, if the primary and secondary are statistically indistinguishable, and the secondary is located at phase 0.5, then it is possible that the TCE is a PC that has been detected at twice the true orbital period. Thus, the robovetter labels a TCE with a significant secondary as a PC when ${\sigma_{\rm Pri} - \sigma_{\rm Sec} < FA_{2}}$ and the phase of the secondary is within 1/4 of the primary transit's duration of phase 0.5. Second, hot Jupiter PCs can have detectable secondary eclipses due to planetary occultations via reflected light and thermal emission \citep{Coughlin2012}. Thus, a TCE with a detected significant secondary is labeled as a PC with the significant secondary flag (in order to facilitate the identification of hot Jupiter occultations) when the geometric albedo is less than 1.0, the planetary radius is less than 30~\re{}, the depth of the secondary is less than 10\% of the primary, and the impact parameter is less than 0.95. The additional criteria beyond the albedo criterion are needed to ensure that this test is only applied to potentially valid planets and not grazing eclipsing binaries. We calculate the geometric albedo by using the stellar mass, radius, and effective temperature from \citet{Huber2014a}, and the values of the period and radius ratio from the DV module of the \kepler{} pipeline.



\paragraph{Odd/Even Depth Difference}
\label{s:oddeven}
If the primary and secondary eclipses of an EB are similar in depth, and the secondary is located near phase 0.5, the \kepler{} pipeline may detect them as a single TCE at half the true orbital period of the EB. In these cases, if the primary and secondary depths are dissimilar enough, it is possible to detect it as a FP by comparing the depths of the odd- and even-numbered transit events. Thus, we compute the following statistic, for both the DV and alternate detrending,

\begin{equation}
\sigma_{\rm OE} = \frac{d_{\rm odd} - d_{\rm even}}{\sqrt{\sigma_{odd}^{2} + \sigma_{even}^{2}}} 
\end{equation}

\noindent where $d_{\rm odd}$ is the median depth of the odd-numbered transits, $d_{\rm even}$ is the median depth of the even-numbered transits, $\sigma_{odd}$ is the standard deviation of the depths of the odd-numbered transits, and $\sigma_{even}$ is the standard deviation of the depths of the even-numbered transits. For the alternate detrending with a trapezoidal fit, we use all points that lie within $\pm$30 minutes of the central time of transit, as well as any other points within the in-transit flat portion of the trapezoidal fit. For the DV detrending, we use all points within $\pm$30 minutes of the central time of transit. (This threshold corresponds to the long-cadence integration time of the \kepler{} spacecraft. Including points farther away from the central time of transit degrades the accuracy and precision of the test.) If $\sigma_{\rm OE}$ $>$ 1.7 for either the DV or alternate detrending then the TCE is labeled as a FP due to a significant secondary. The value of 1.7 was empirically derived utilizing manual checks and transit injection.


\subsubsection{Out of Eclipse Variability}

Short-period eclipsing binaries will often show out-of-eclipse variability due to tidal forces that deform the star from a perfect spheroid. The variability manifests as quasi-sinusoidal variations at either the period, or half the period, of the binary.

We use the information from SWEET (see~\S\ref{s:sweetntl}) to detect these cases. If a transit-like TCE has a SWEET SNR greater than 50, an amplitude less than the TCE transit depth in either the DV and ALT detrendings, an amplitude greater than 5,000~ppm, and a period less than 10 days, we fail it as a stellar system.



\subsubsection{Shape Metric}

There are cases of EBs that do not show a secondary eclipse, either due to the secondary star being too low luminosity for the eclipse to be detectable, or the binary has significant eccentricity and a longitude of periastron such that geometrically no eclipse occurs. Also, most detached EBs will not exhibit detectable out-of-eclipse variability. In these cases, the only remaining way to detect that the signal is due to a stellar system is to utilize the shape and depth of the transit to infer that the signal could not possible be due to a planet.

In previous catalogs \citep{Rowe2015cat,Mullally2015cat,Coughlin2016} TCEs were not failed based on their inferred radii alone. This was on purpose as the catalogs attempted to be as agnostic to stellar parameters as possible, such that dispositions would remain applicable if and when better stellar parameters were obtained, e.g., by GAIA \citep{Cacciari2009,Mignard2005}. This resulted in some PC KOIs with large depths that were known to very likely be EBs, and in fact were later confirmed as such by follow-up observations \citep{Santerne2016}.

In this catalog, we attempt to strike a balance between identifying these binary systems, while still remaining agnostic to stellar parameters. We adapted a simple shape parameter, originally used in \citet{Batalha2013}, and express it as the sum of the modeled radius ratio and the impact parameter. This metric reliability identifies eclipsing binaries both due to being too deep (large $R_{p}$/$R_{\star}$) and due to grazing eclipses (large $b$). Specifically we fail a transit-like TCE as a stellar system if $R_{p}$/$R_{\star}$~+~$b$~$>$~1.04.



\subsection{Centroid Offset}
\subsubsection{Centroid Robovetter}
The Robovetter relies on a piece of code called the Centroid Robovetter \footnote{REPOSITORY}\citep{KSCI}[CITE the CRV KSCI] to detect when a transit signal originates from a background or nearby star instead of from the target star. This Centroid Robovetter has not changed since its implementation for the DR24 KOI catalog. We summarize it here. 

Given that \kepler 's pixels are 3.98\arcsec{} square \citep{Koch2010}, and the typical photometric aperture has a radius of 4--7 pixels \citep{Bryson2010b}, it is quite common for a given target star to be contaminated by light from another star. If that other star is variable, then that variability will be visible in the target aperture at a reduced amplitude. If the variability due to contamination results in a TCE, then it is a false positive, whether the contaminator is an eclipsing binary, planet, or other type of variable star \citep{Bryson2013}. For example, if a transit or an eclipse occurs on a bright star, a shallower event will be observed on a nearby, fainter star. Similarly, a star can be mistakenly identified as experiencing a shallow transit if a deep eclipse occurs on a fainter, nearby source.

% There are two ways we robotically detect centroid offsets, by examining the differnce 


The DV module of the \kepler{} pipeline produces ``difference images'' for each quarter, which are made by subtracting the average flux in each pixel during each transit from the flux in each pixel just before and after each transit \citep{Bryson2013}. If the resulting difference image shows significant flux at a location (centroid) other than the target, then the TCE is likely an FP due to a centroid offset.


% In prior catalogs, TCERT members manually examined the difference images to look for evidence of a centroid offset, as fully described in \citet{Bryson2013} and \S3.2.3--3.2.6 of \citet{Rowe2015a}. In this catalog, the search for centroid offsets was fully robotized and confirmed to reproduce the results earlier catalogs using human vetting \citep{Mullally2015c}.

In our robotic procedure to detect FPs due to centroid offsets, we first check that the difference image for each quarter contains a discernible stellar image and is not dominated by background noise. This is done by searching for at least 3 pixels that are adjacent to each other and brighter than a given threshold, which is set by the noise properties of the image. We use an iterative sigma clipping approach to eliminate bright pixels when calculating the background noise, as the star often dominates the flux budget of a substantial number of pixels in the aperture.

For the difference images that are determined to contain a discernible stellar image, we first search for evidence of contamination from sources that are resolved from the target. Since resolved sources near the edge of the image may not be fully captured, Pixel Response Function (PRF --- \keplers{} point spread function convolved with the image motion and the intra-pixel CCD sensitivity) fitting approaches do not often work well to detect them. Instead, we check if the location of the brightest pixel in the difference image is more than 1.5 pixels from the location of the target star. If at least two-thirds of the quarterly difference images show evidence of an offset by this criterion, we disposition the TCE as a FP due to a centroid offset. % Note that FPs due to stars located many pixels from the target, i.e., far outside the target's image, are not detected by this approach, but rather through ephemeris matching (see \S\ref{ephemmatchsec}).

If no centroid offset is identified by the previous method, we then look for contamination from sources that are unresolved from the target. We measure the PRF-fit centroid of the difference images and search for statistically significant shifts with respect to the PRF centroid of both the out-of-transit images, as well as the catalog position of the source. Following \citet{Bryson2013}, a TCE is marked as a FP due to a centroid offset if there are at least three difference images with a discernible stellar image, and a 3$\sigma$ significant offset larger than 2$\arcsec$, or a 4$\sigma$ offset larger than 1$\arcsec$ is measured.  If any of these are not true, a flag is set indicating that fact.

The Centroid Robovetter gives the \kepler\ Robovetter several flags to indicate whether a centroid offset was detected and whether that detection can be trusted. The names of those flags have been changed to be consistent with our minor flag naming scheme. A list of the minor flags are available in Appendix~\ref{s:minorflags}.

%We use the the pixel-level injected TCEs that were injected with an offset to understand the performance of the Centroid Robovetter.  [FERGAL IT WOULD BE NICE IF I COULD GET YOU TO WRITE THIS PART and include a figure. BUT it is a nice to have.]

\subsubsection{Ghost Diagnostic}
The last method we use to detect a centroid offset is the ghost diagnostic, which was added to the DR25 \kepler{} Pipeline. It determines whether a transit signal is likely to be contaminated from a ghost image of a star located away from the target star in the focal plane. Ghost reflections occur when light from a bright star is reflected from the CCD reflects again from the field flattener plate and back onto the CCD as a diffuse, out-of-focus image of the pupil of
the telescope. A similar type of false positive results from direct PRF (Pixel Response Function) contamination, when flux from the broad wings of the PRF of a bright star near the target star on the CCD overlaps the target star's PRF.  If a ghost reflection (or the PRF of a nearby star) containing a transit-like signature (e.g. an eclipsing binary signal) overlaps the PRF of a target star, then the contaminating transit signal will be significantly stronger relative to the target's flux in the periphery of the target star's PRF than in its core.

To detect this type of false alarm, the ghost diagnostic essentially measures the strength of the TCE signal in two separate light curves --- one created using the average of the pixels inside the target's optimal aperture minus the average of the pixels in an annulus surrounding the target aperture (core aperture correlation statistic), and the other using the average of the pixels in the annulus surrounding the target aperture (halo aperture correlation statistic). If the ratio of the out- to in-aperture statistic is greater than 4.0, the TCE is marked as a FP due to a centroid offset. This ghost diagnostic is not available for vetting the \scrtce s. 

%Joe's words
% It produces a detection statistic measuring correlation between a whitened timeseries and a whitened transit model, as well as a statistic measuring the signicance of the correlation, assuming the detection statistic is a chi-squared variable.

\subsection{Ephemeris Matching}
\label{ephemmatchsec}

Another method for detecting FPs due to contamination is to compare the ephemerides (periods and epochs) of TCEs to each other, as well as other known variable sources in the \kepler{} field. If two targets have the same ephemeris within a specified tolerance, then at least one of them is a FP due to contamination. \citet{Coughlin2014a} used Q1--Q12 data to compare the ephemerides of KOIs to each other and eclipsing binaries known from both \kepler{}- and ground-based observations. They identified over 600 FPs via ephemeris matching, of which over 100 were not known as FPs via other methods. They also identified four main mechanisms of contamination. The results of \citet{Coughlin2014a} were incorporated in \citet[][see \S3.3]{Rowe2015a}. \citet[][see \S5.3]{Mullally2015cat} slightly modified the ephemeris matching process of \citet{Coughlin2014a}, and applied it to all of the Q1--Q16 TCEs, as well as known KOIs and EBs, identifying nearly 1,000 TCEs as FPs. The same process was again used in the \citet{Coughlin2016} catalog.

We modify the matching criteria used in previous catalogs to obtain more accurate matches, due especially to the very large number of long-period TCEs encountered in DR25 compared to previous activities. We utilize the results of the transit injection run (\S\ref{s:injectsec}) to measure the ability of the \kepler{} pipeline to recover period and epoch as a function of period. In Figure~\ref{injephemfig} we show, in the top two panels, the difference in the injected and recovered period and epoch, as a function of the injected period. The middle and bottom panels show the measured standard deviation of the difference as a function of panel, in linear and logarithmic space respectively. The red line is the result of a best-fit power law.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{DR25-Injected-Vs-Recovered-Periods-And-Epochs.png}
\caption{A plot of injected vs. recovered periods and epochs of injected on-target planets. MAYBE UPDATE TO JUST 2x2 WITH ALL LOG-SCALE - I.E. KEEP BOTTOM PLOTS, TAKE OUT MIDDLE, AND MAKE TOP LOGSCALE.}
\label{injephemfig}
\end{figure}


When comparing two objects, A and B, where A is defined to have the shorter period, the new matching metrics we use, $S_{P}$ and $S_{T}$ for period and epoch respectively, are:

\begin{equation}
    S_{P} = \frac{\left|P_{r} \cdot P_{A} - P_{B}\right|}{\sqrt{2}\cdot\sigma_{P}(P_{A})}
\end{equation}

\begin{equation}
    S_{T} = \frac{\left| T_{A} - T_{B} - T_{r} \cdot P_{A}\right|}{\sqrt{2}\cdot\sigma_{T}(P_{A})}
\end{equation}

\noindent where $P_{A}$ and $P_{B}$ are the periods of objects A and B, $T_{A}$ and $T_{B}$ are similarly the epochs of objects A and B, $\sigma_{P}(P_{A})$ and $\sigma_{T}(P_{A})$ are the errors in period and epoch, given period $P_{A}$, derived from the best-fit power law to the standard deviation of the injected vs recovered periods and epochs, respectively, and the period ratio, $P_{r}$, and epoch ratio, $T_{r}$, are defined by:

\begin{equation}
P_{r} = \textrm{rint}\left(\frac{P_{A}}{P_{B}}\right)
\end{equation}

\begin{equation}
T_{r} = \textrm{rint}\left(\frac{T_{A} - T_{B}}{P_{A}}\right)
\end{equation}


\noindent where $rint()$ rounds a number to the nearest integer. Thus, a perfect match has $S_{P}$~=~0 and  $S_{T}$~=~0, with worse matches having increasingly larger values of $ \sigma_{P}$ and $ \sigma_{T}$. 

We consider matches with $S_{P}$~$<$~5 and $S_{T}$~$<$~5, with period ratios of 50 or less ($P_{r}$~$<$~50), to be statistically significant enough to constitute a match. We also require:

\begin{enumerate}

\item The two objects can not have the same KIC ID.

\item The two objects have to satisfy at least one of the following conditions: 

    \begin{enumerate}
    
    \item Have a separation distance of less than $d_{\rm max}$ arcseconds of each other, where
    \begin{equation}
    \label{disteq}
    d_{\rm max}(\arcsec) = 55\cdot\sqrt{10^{6}\cdot 10^{-0.4 \cdot m_{\rm kep}}+1}
    \end{equation}

\noindent and where the magnitude of the brighter source is used for $m_{\rm kep}$.  

    \item Be located on equidistant, opposite sides of the FoV center within a 100$\arcsec$ (25 pixel) tolerance.
    
    \item Be located on the same CCD module and be within 5 pixels of the same column value, for any of the 4 quarters.
    
    \item Be located on the same CCD module and be within 5 pixels of the same row and column value, for any of the 4 quarters.

   \end{enumerate}

\end{enumerate}


\noindent Criterion 1 ensures that no star is ever matched to itself. Criterion 2a is a semi-empirically determined formula derived to account for direct PRF contamination and reflection off the field flattener lens, assuming the average wings of a \emph{Kepler} PSF can be approximated by a Lorentzian distribution. The formula allows for any two stars to match within a generous 55$\arcsec$ range, but allows for bright stars to match to larger distances, e.g., a 10$^{\rm th}$ mag star could match up to 550$\arcsec$ away, and a 5th mag star could match up to 5500$\arcsec$ away. Criterion 2b accounts for antipodal reflection off the Schmidt Corrector. Criterion 2c accounts for the column anomaly \citep[see \S3.5 of][]{Coughlin2016}, and criterion 2d accounts for CCD crosstalk.


In this Q1--Q17~DR25 catalog, we match the ephemerides of all Q1--Q17~DR25 TCEs \citep{Twicken2016} to the following sources:

\begin{itemize}
 \item Themselves.
 \item The list of \npredrtwentyfivekois{} KOIs from the NASA Exoplanet Archive cumulative KOI table after the closure of the Q1--Q17~DR24 table and publication of the last catalog \citep{Coughlin2016}.
 \item The \kepler{} EBWG of \nkebs{} true EBs found with \kepler{} data as of 2016 October 13 \citep{Prsa2011,Slawson2011,Kirk2016}.
 \item J.M. Kreiner's up-to-date database of ephemerides of ground-based eclipsing binaries as of 2016 October 13 \citep{Kreiner2004}.
 \item Ground-based eclipsing binaries found via the TrES survey \citep{Devor2008a}.
 \item The General Catalog of Variable Stars \citep[GCVS][]{Samus2015} list of all known ground-based variable stars, published 2016 October 05.
\end{itemize}







% - I better implemented bastard detection, when an object's best match (taking into account the deepest object it can possibly match to) results in a parent can't physically be the parent. I cleaned up the PRF model fit - see attached figure [dmag-dist-drat.png] - and to document for the paper later on, I did a 4.0 sigma outlier rejection for a robust fit. I also fully automated and improved the process to make sure all bastards and recorded in the final results file.


% - Since we now better understand column anomalies and antipodal reflection, I've also added a bastard flag to column anomalies for objects that match with their depths within a factor of 100 (i.e., 0.01 < depth1/depth2 < 100). All the real column anomalies with definitively identified parents have depth reductions of 10^3 - 10^5.


% - I made no effort to specifically detect ~370 day rolling band TCEs, but I didn't filter them out either. We end up picking up quite a few though, mostly from the worst rolling-band channels.


% - Overall I fail 1,859 OPS, 471 INV, 459 SS1, and 29 planet injections (23 on-target, 6 off-target). INV and SS1 produce real matches mostly due to short-period things like RR-Lyr that still produce lots of TCEs in INV and SS1.


% - There's only one confirmed planet that fails ephemeris matching: TCE 005175986-01/KOI 2708.01. It's a 0.868 day object with a depth of 90ppm that was confirmed due to Morton's FPP analysis. It has a 1:1 period match with TCE 005088308-03 due to direct PRF, which was classified as a bastard match. Upon inspection it is likely this is a coincidence match, as the shapes are different. It's possible with more tweaking I could eliminate this match, but it's pretty well within the matching criteria.






Via ephemeris matching, we identify \nephemmatch{} Q1--Q17~DR25 TCEs as FPs. Of these, \nonlyephemmatch{} were identified as FPs only due to ephemeris matching. We list all \nephemmatch{} TCEs in Table~\ref{ephemmatchtab}, as this information is valuable for studying contamination in the \kepler{} field. (Note that each TCE identified consists of its KIC ID and planet number, separated by a dash.) We also list in Table~\ref{ephemmatchtab} each TCE's most likely parent, the period ratio between child and parent (P$_{\rm rat}$), the distance between the child and parent in arcseconds, the offset in row and column between the child and parent in pixels ($\Delta$Row and $\Delta$Col), the magnitude of the parent (m$_{\rm Kep}$), the difference in magnitude between the child and parent ($\Delta$Mag), the depth ratio of the child and parent (D$_{\rm rat}$), the mechanism of contamination, and a flag to designate unique situations. In Figure~\ref{ephemmatchfig} we plot the location of each false positive TCE and its most likely parent, connected by a solid line. TCEs are represented by solid black points, KOIs are represented by solid green points, EBs found by \kepler{} are represented by solid red points, EBs discovered from the ground are represented by solid blue points, and TCEs due to a common systematic are represented by open black points. The \kepler{} magnitude of each star is shown via a scaled point size. Note that most parent-child pairs are so close together that the line connecting them is not easily visible on the scale of the plot. 


Since \kepler{} does not observe every star in its field of view, it can often be the case that a match is found between two objects, but given their relative magnitude, distance, and depths it is clear that neither is the parent of the other, and so are classified as bastards \citep{Coughlin2014a}. To identify these cases for the mechanism of direct PRF contamination, we performed a robust fit of the Kepler PRF model described by equations~9~and~10 of \citet{Coughlin2014a} to the depth ratio, magnitude difference, and distance between each object identified as due to direct PRF contamination and its most likely parent. After iteratively rejecting outliers greater than 4.0 times the standard deviation the fit converged with values of $\alpha$ = 6.93\arcsec and $\gamma$ = 0.358\arcsec. Outliers greater than 4.0 times the standard deviation of the final iteration, with these resulting fit parameters, were labeled as bastards, For the mechanism of column anomaly and reflection, if the depth ratio of the two objects is between 0.01 and 100, then it is labeled a bastard, as these mechanisms should produce depth ratios of at least 1E-3 or 1E3. All bastards are identified with a flag of 1 in Table~\ref{ephemmatchtab}. Additionally, it can sometimes be the case that objects are matched via the column anomlay, but are on different outputs of the same module --- these cases likely inovlve the column anomaly working in conjunction with cross-talk, and thus are complicated, and given a flag of 2 in Table~\ref{ephemmatchtab}. Finally, a flag of 3 indicates a combination of flags 1 and 2. 


\begin{deluxetable*}{ccccccccccc}
\tablecolumns{11}
\tabletypesize{\scriptsize}
\tablewidth{\linewidth}
\tablecaption{The \nephemmatch{} Q1--Q17~DR25 TCEs Identified as FPs due to Ephemeris Matches}
\tablehead{\colhead{TCE} & \colhead{Parent} & \colhead{P$_{\rm rat}$} & \colhead{Distance} & \colhead{$\Delta$Row} & \colhead{$\Delta$Col} & \colhead{m$_{\rm Kep}$} & \colhead{$\Delta$Mag} & \colhead{D$_{\rm rat}$} & \colhead{Mechanism} & \colhead{Flag} \\ & & & (\arcsec) & (Pixels) & (Pixels) & & & & & }
\input{ephemmatch-stub.tex}
\enddata
\tablecomments{A suffix of ``pri'' in the parent name indicates the object is an EB known from the ground, and the child TCE matches to its primary. Similarly a suffix of ``sec'' indicates the child TCE matches the secondary of a ground-based EB. Parent names are listed, in priority order when available, by (1) their Bayer designation (e.g., RR-Lyr-pri), (2) their EBWG designation (e.g., 002449084-pri), (3) their KOI number (e.g., 3924.01), and (4) their TCE number (e.g., 001724968-01). A flag of 1 indicates that the TCE is a bastard, which are cases where two or more TCEs match each other via the Direct-PRF contamination mechanism, but neither can physically be the parent of the other via their magnitudes, depths, and distances, and thus the true parent has not been identified. A flag of 2 indicates cases of column anomalies that occur on different outputs of the same module. These cases likely involve cross-talk to carry the signal from one output to another. TCEs due to the common systematic do not have information listed for a parent source, as they are not caused by a single parent. Note that  Table~\ref{ephemmatchtab} is published in its entirety in the electronic edition of the Astrophysical Journal. A portion is shown here for guidance regarding its form and content.}
\label{ephemmatchtab}
\end{deluxetable*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{CCDPlot.pdf}
\caption{Distribution of ephemeris matches on the focal plane. Symbol size scales with magnitude, while color represents the catalog in which the contaminating source was found. Blue indicates that the true transit is from a variable star only known as a result of ground-based observations. Red circles are stars listed in the \kepler{} EBWG catalog, green are KOIs, and black are TCEs. Black lines connect false positive matches with the most likely contaminating parent. In most cases parent and child are so close that the connecting line is invisible.}
\label{ephemmatchfig}
\end{figure*}


% The larger number of matches compared to the Q1--Q12 and Q1--Q16 catalogs is predominately the result of a much larger short-period false positive population compared to Q1--Q16, and an extended baseline compared to Q1--Q12, coupled with matching all TCEs and not just KOIs. In Q1--Q17~DR24 we identify an additional contamination mechanism, which we label ``Common Systematic''. As mentioned in \S\ref{tcesec}, these are over 200 TCEs that are caused by 3 systematic events that are common to all \kepler{} CCDs and happen to be equidistant in time with a spacing of $\sim$459 days. 

% We also identify 119 examples of ``Column Anomaly'', which is a previously identified mechanism where a parent is able to contaminate a child at large distances if they both lie on the same column of a CCD. This mechanism is particularly pernicious because it does not result in a visible centroid offset; the apparent location of the transit signal via the difference images coincides with the target. If the parent is not observed by \kepler{}, then the child could go undetected as a FP due to the column anomaly, as was recently the case for KOI~6705.01 \citep{Gaidos2015}. The large number of examples of column anomaly now available in the Q1--Q17~DR24 catalog reveals the following:

% \begin{itemize}

% \item Despite equally searching for matches in row and column, no instance of ``row-anomaly'' has been found to occur.

% \item The CCDs are read out in the column direction.

% \item In 91.6\% of cases, the child is at a higher row number than the parent, and thus the parent's pixels are read out before the child's. (The remaining 8.4\% of cases may not have the true parent identified, but rather a sibling, as only the most likely parent is listed, and many parents are unobserved by the spacecraft.)

% \item Most cases show the depth of the child increases over time.

% \item The effect appears to exhibit seasonal depth variations in most cases.

% \item The average depth ratio between parent and child is a factor of $\sim$$10^{4}$, and typically the parent and child have similar magnitudes.

% \end{itemize}

% \noindent Combining these details leads to our conjecture that the column anomaly is due to decreasing charge transfer efficiency over time, likely due to cosmic ray impacts. When the CCD is read out, some charge from the parent is left behind due to charge transfer inefficiency. As the child is read out, and its electrons pass through the pixels where the parent was, the child picks up some of the parent's left-behind electrons. Thus, the variable signal from the parent is induced in the child. As more cosmic ray impacts accumulate over time, the amount of charge left behind by the parent increases, resulting in an increase in contamination, and thus an increase in the observed depth of the child. Seasonal variation is seen as the parent and child rotate between 4 CCDs with season, and the amount of degradation varies with CCD. The average depth ratio, along with the delta magnitudes observed, indicate that a charge transfer efficiency of $\sim$99.99\% is consistent with the observed contamination, i.e., a degradation of $\sim$0.01\%. This is well within the range observed on Hubble's Advanced Camera for Surveys and other spaceborne detectors  \citep[see \S3.7 of][and references therein]{Sirianni2005}.


\subsection{Disposition Scores}
\label{s:scores}
A new feature we introduce in this catalog is the disposition score --- a value between 0 and 1 that indicates the confidence in disposition provided by the Robovetter. A higher valued indicates more confidence that an objects is a PC, regardless of the disposition it was given.  This feature allows one to select the highest quality PCs by ranking KOIs by the disposition score. 
%This statistics argument hasn't been proven. We could discuss this, but it belongs in the disucssion "and weight them appropriately for their statistics."

The disposition score was calculated by wrapping the Robovetter in a Monte Carlo routine. In each of 10,000 iterations the input metrics for the Robovetter are perturbed from their nominal values by drawing from an asymmetric Gaussian distribution centered on the nominal value. In each iteration the robovetter dispositions each TCE given the new values for each metric. The disposition score is simply the fraction of iterations that result in a disposition of PC. For example, if a PC KOI has a few metrics that are near the Robovetter's thresholds, it will frequently have at least one that is perturbed across a threshold. As a result, many of the iterations will produce a false positive and the candidate will have a low score.  Similarly, if a FALSE POSITIVE KOI barely fails a single metric, the score may be near 0.5, indicating that it was deemed a PCin half of the iterations.

To compute the asymmetric sigma values for the Gaussian distribution, we examine the distribution of the robovetter metric for the injected on-target population. We calculate the positive and negative median absolute deviation (MAD) values for each metric as a function of MES and period. These MAD values are then multiplied by a conversion factor of 1.4826 (ADD REF) to put the variability on the same scale as a standard deviation.

NEED TO MENTION AND CLEAN UP FIGURES

NEED TO LIST WHICH METRICS actually GO INTO THE Score CALCULATION

TALK ABOUT SCORE DISTRIBUTION HERE, OR LATER ON IN ANALYSIS?

\begin{figure}
\centering
\includegraphics[width=\linewidth]{Metric-StDev-1.pdf}
\caption{A plot of how we calculated errors for the robovetter score.[NEEDS MORE WORK]}
\label{score-fig-1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{Scores-1.pdf}
\caption{A plot of how we calculated errors for the robovetter score.}
\label{score-fig-2}
\end{figure}

\subsection{Summarize Metrics}
All the metrics, the major and minor flags they create and the thresholds used are summarized in Table~\ref{t:metrics}. For those with easily tunable thresholds, the value used to create the DR25 KOI catalog are listed.


\input{metrics-table}